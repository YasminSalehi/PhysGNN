{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6OLsMgB9xKr"
   },
   "source": [
    "# Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PzQa4nBFZv1"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOTkcxQ0QNI3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9uXK3Hgxqyl"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfPS93SRxubn"
   },
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uv4ydHi3xxIK"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hHaTCH1xz8v"
   },
   "outputs": [],
   "source": [
    "%cd PhysGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkf9vx6lXom4"
   },
   "source": [
    "# Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyY9b8dausZY"
   },
   "outputs": [],
   "source": [
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
    "!pip install tensorboardX\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4O_TNMc91_d"
   },
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmkZgCfAcpvw"
   },
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "# Importing what we need\n",
    "import random \n",
    "from random import shuffle\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "import datetime\n",
    "import pickle \n",
    "from joblib import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU, GRU\n",
    "from torch_geometric.nn import JumpingKnowledge\n",
    "\n",
    "import torch_geometric as tg\n",
    "import torch_geometric.nn as tg_nn\n",
    "import torch_geometric.utils as tg_utils\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.nn import Sequential, JumpingKnowledge\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "from torch.nn import init\n",
    "import pdb\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "# For visualizing the results\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtPIP85e-suR"
   },
   "source": [
    "# Data set generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHWsN8vSffjx"
   },
   "source": [
    "##Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD_sda0EyQok"
   },
   "outputs": [],
   "source": [
    "def Graph_load_batch(name, min_num_nodes = 20, max_num_nodes = 9118):\n",
    "\n",
    "    \"\"\"\n",
    "    load many graphs\n",
    "    :return: a list of graphs\n",
    "    \"\"\"\n",
    "    print('Loading graph dataset: ' + str(name))\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # load data\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    print('Loading the adjacency matrix...')\n",
    "    data_adj = np.loadtxt(name+'/A.csv', delimiter=',').astype(int)\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    print('Loading the graph indicator...')\n",
    "    data_graph_indicator = np.loadtxt(path+'graph_indicator.csv', delimiter=',').astype(int)\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    print('Loading the graph labels...')\n",
    "    data_graph_labels = np.loadtxt(path+'graph_labels.csv', delimiter=',').astype(int)\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    print('Loading the node attributes...')\n",
    "    data_node_att = np.loadtxt(path  + 'node_attributes_raw.csv', delimiter=',')\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    print('Loading the node labels...')\n",
    "    node_labels = np.loadtxt(path + 'output_displacement.csv', delimiter=',')\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # #######################################################################################\n",
    "    print('Data loaded.')\n",
    "    # #######################################################################################\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    print('Generating data tuples...')\n",
    "    data_tuple = list(map(tuple, data_adj))\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    print('Adding edges...')\n",
    "    G.add_edges_from(data_tuple)\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    print('Adding features and node labels to graph nodes...')\n",
    "    for i in range(node_labels.shape[0]):\n",
    "        G.add_node(i + 1, feature=data_node_att[i])\n",
    "        G.add_node(i + 1, label=node_labels[i])\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    print('Removing isolated nodes...')\n",
    "    print(list(nx.isolates(G)))\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    print('Splitting data into graphs...')\n",
    "    graph_num = data_graph_indicator.max()\n",
    "    node_list = np.arange(data_graph_indicator.shape[0]) + 1\n",
    "    graphs = []\n",
    "    max_nodes = 0\n",
    "    for i in range(graph_num):\n",
    "        # find the nodes for each graph\n",
    "        nodes = node_list[data_graph_indicator == i + 1]\n",
    "        G_sub = G.subgraph(nodes)\n",
    "        G_sub.graph['label'] = data_graph_labels[i]\n",
    "        if G_sub.number_of_nodes() >= min_num_nodes and G_sub.number_of_nodes() <= max_num_nodes:\n",
    "            graphs.append(G_sub)\n",
    "            if G_sub.number_of_nodes() > max_nodes:\n",
    "                max_nodes = G_sub.number_of_nodes()\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    print('Loaded')\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5qPHdKFfo7L"
   },
   "source": [
    "##Pytorch Geometric Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03EtjnPmfpK8"
   },
   "outputs": [],
   "source": [
    "def nx_to_tg_data(graphs):\n",
    "    \n",
    "    data_list = []\n",
    "\n",
    "    for i in range(len(graphs)):\n",
    "\n",
    "        graph = graphs[i].copy()\n",
    "\n",
    "        # relabel graphs\n",
    "        keys = list(graph.nodes)\n",
    "        vals = range(graph.number_of_nodes())\n",
    "        mapping = dict(zip(keys, vals))\n",
    "        nx.relabel_nodes(graph, mapping, copy=False)\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        feature_values = nx.get_node_attributes(graph, 'feature').values()\n",
    "        label_values = nx.get_node_attributes(graph, 'label').values()\n",
    "        num_nodes = len(feature_values)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Node attribute format\n",
    "        # {x} {y} {z} {physical_property} {F_x} {F_y} {F_z}  \n",
    "        j = 0\n",
    "        features = []\n",
    "        for value in feature_values:\n",
    "            if j == 0:\n",
    "                features = value\n",
    "            else:\n",
    "                features = np.concatenate((features, value), axis=0)\n",
    "            j += 1\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Node label format \n",
    "        # {d_x} {d_y} {d_z}\n",
    "        j = 0\n",
    "        node_labels = []\n",
    "        for value in label_values:\n",
    "            if j == 0:\n",
    "                node_labels = value\n",
    "            else:\n",
    "                node_labels = np.concatenate((node_labels, value), axis=0)\n",
    "            j += 1\n",
    "        # ----------------------------------------------------------------------\n",
    "        features = features.reshape((num_nodes, 7)) # because we have 7 features\n",
    "        features = torch.from_numpy(features).float()\n",
    "\n",
    "        node_labels = node_labels.reshape((num_nodes, 3)) # because we have 3 outputs\n",
    "        node_labels = torch.from_numpy(node_labels).float() \n",
    "\n",
    "        pos = features[:, 0:3]\n",
    "        x = features[:, 3:]\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # get edges\n",
    "        edge_index = np.array(list(graph.edges))\n",
    "        edge_index = np.concatenate((edge_index, edge_index[:,::-1]), axis=0)\n",
    "        edge_index = torch.from_numpy(edge_index).long().permute(1,0)\n",
    "\n",
    "        print(edge_index)\n",
    "\n",
    "        # get edge_labels\n",
    "        edge_values = np.ones((edge_index.shape[1], 1))\n",
    "        edge_values = torch.tensor(edge_values).float()\n",
    "\n",
    "        for n in range(edge_index.shape[1]):\n",
    "            node_1 = edge_index[0,n]\n",
    "            node_2 = edge_index[1,n]\n",
    "\n",
    "            x_1 = pos[node_1, 0]\n",
    "            y_1 = pos[node_1, 1]\n",
    "            z_1 = pos[node_1, 2]\n",
    "\n",
    "            x_2 = pos[node_2, 0]\n",
    "            y_2 = pos[node_2, 1]\n",
    "            z_2 = pos[node_2, 2]\n",
    "\n",
    "            edge_values[n, 0] = torch.pow((torch.pow((x_1 - x_2), 2) + torch.pow((y_1 - y_2), 2) + torch.pow((z_1 - z_2), 2)), 0.5)\n",
    "        \n",
    "        edge_values = torch.tensor(edge_values).float()\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Checking for dimensionality correctness \n",
    "        print(pos.shape)\n",
    "        print(pos)\n",
    "        print(node_labels.shape)\n",
    "        print(node_labels)\n",
    "        print(x.shape)\n",
    "        print(x)\n",
    "        # ----------------------------------------------------------------------\n",
    "        # create the data object\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_values, pos=pos, y=node_labels)\n",
    "\n",
    "        data_list.append(data)\n",
    "\n",
    "        # Progress update\n",
    "        print(str(i+1)+'/'+str(len(graphs))+' data objects created.')\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lGfrUAefWRC"
   },
   "source": [
    "#Final Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-SlMyzGFhiw"
   },
   "outputs": [],
   "source": [
    "dataset_name = 'dataset_1'\n",
    "# dataset_name = 'dataset_2'\n",
    "\n",
    "# path = dataset_name + '/a/' # \n",
    "# path = dataset_name + '/b/' # \n",
    "# path = dataset_name + '/c/' # \n",
    "# path = dataset_name + '/d/' # \n",
    "# path = dataset_name + '/e/' # \n",
    "# path = dataset_name + '/f/' # \n",
    "# path = dataset_name + '/g/' # \n",
    "# path = dataset_name + '/h/' # \n",
    "# path = dataset_name + '/i/' # \n",
    "# path = dataset_name + '/j/' # \n",
    "# path = dataset_name + '/k/' # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBrKOhcd_s43"
   },
   "outputs": [],
   "source": [
    "graphs_all = Graph_load_batch(dataset_name)\n",
    "dataset = nx_to_tg_data(graphs_all)\n",
    "print('Pytorch Geometric dataset has been created.')\n",
    "\n",
    "# path_save = dataset_name + '_pickle/' + dataset_name + '_a_raw.pickle' #  \n",
    "# path_save = dataset_name + '_pickle/' + dataset_name + '_b_raw.pickle' # \n",
    "# path_save = dataset_name + '_pickle/' + dataset_name + '_c_raw.pickle' # \n",
    "# path_save = dataset_name + '_pickle/' + dataset_name + '_d_raw.pickle' # \n",
    "# path_save = dataset_name + '_pickle/' + dataset_name + '_e_raw.pickle' # \n",
    "# path_save = dataset_name + '_pickle/' + dataset_name + '_f_raw.pickle' # \n",
    "# path_save = dataset_name + '_pickle/' + dataset_name + '_g_raw.pickle' # \n",
    "# path_save = dataset_name + '_pickle/' + dataset_name + '_h_raw.pickle' # \n",
    "# path_save = dataset_name + '_pickle/' + dataset_name + '_i_raw.pickle' # \n",
    "# path_save = dataset_name + '_pickle/' + dataset_name + '_j_raw.pickle' # \n",
    "# path_save = dataset_name + '_pickle/' + dataset_name + '_k_raw.pickle' # \n",
    "\n",
    "torch.save(dataset, path_save)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of Dataset_Generation_V2.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "n6OLsMgB9xKr",
    "pkf9vx6lXom4",
    "D4O_TNMc91_d",
    "AXDJVudFDJNL",
    "momkH5ONC-0J",
    "DSri18MsCs42",
    "F0b4jrDzCjx8",
    "6_Eii4TqCSkI",
    "8VxsLOrPCI6m",
    "DlS8RzY-CAoh",
    "pmT97UrCB2j9",
    "P2TrDBKACdPR",
    "qs4S-h2C32Zs",
    "2fgfWYk4-fM1",
    "6HnFr9_7BuMB",
    "GH-VDv8FBc4d",
    "GFEf8O_sAR9y",
    "nOBmluPm-fFL",
    "Bs7x1OwM-mcZ",
    "vtPIP85e-suR",
    "thPP8i75HOIr",
    "ma0wDlBLnlgg",
    "Y3uGgN0gnrB4",
    "xBzdaJLO_H3q",
    "K03gD1VDrCTq",
    "WkCEhHtP_RjI"
   ],
   "toc_visible": true,
   "machine_shape": "hm",
   "authorship_tag": "ABX9TyOW/EhWok4oAuzWGQR4dmWA"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
